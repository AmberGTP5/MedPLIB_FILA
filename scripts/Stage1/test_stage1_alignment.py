# file: run_inference_fixed.py
#
# æœ€ç»ˆè§£å†³æ–¹æ¡ˆ: ä½¿ç”¨"çŒ´å­è¡¥ä¸"æŠ€æœ¯åœ¨ä¸ä¿®æ”¹æ¨¡å‹æ–‡ä»¶çš„å‰æä¸‹ä¿®å¤æ¨ç†é”™è¯¯ã€‚
# å®ƒä¼šåœ¨è¿è¡Œæ—¶åŠ¨æ€æ›¿æ¢æ¨¡å‹çš„ forward æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿå…¼å®¹æ¨ç†æµç¨‹ã€‚

import os
import sys
from pathlib import Path
import torch
from PIL import Image
from torchvision import transforms
from transformers import AutoConfig, AutoTokenizer, logging as transformers_logging
from functools import wraps

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°ç³»ç»Ÿè·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
# ç¡®ä¿è¿™ä¸ªè·¯å¾„æ˜¯æ­£ç¡®çš„ï¼ŒæŒ‡å‘æ‚¨é¡¹ç›®çš„æ ¹ç›®å½•
# ä¾‹å¦‚ï¼Œå¦‚æœæ­¤è„šæœ¬åœ¨ a/b/c/ä¸‹ï¼Œè€Œæ¨¡å‹ä»£ç åœ¨ a/model/ä¸‹ï¼Œåˆ™è·¯å¾„åº”ä¸º Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(Path(__file__).resolve().parent.parent)) 

from model.fila_lisa import FILAForCausalLM
from utils.utils import DEFAULT_IMAGE_TOKEN, IGNORE_INDEX
from model.medplib.model.language_model.medplib_llama import LlamaForCausalLM

# æŠ‘åˆ¶ä¸å¿…è¦çš„è­¦å‘Šï¼Œä½¿è¾“å‡ºæ›´æ•´æ´
transformers_logging.set_verbosity_error()

# ==============================================================================
# --- âš™ï¸ é…ç½®åŒº (HARDCODED CONFIGURATION) ---
# --- è¯·åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„å®é™…è·¯å¾„ ---
# ==============================================================================
BASE_LLM_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/Llama-3.1-8B-Instruct"
VISION_TOWER_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/clip-vit-large-patch14-336"
STAGE1_WEIGHTS_PATH = "/root/autodl-tmp/MedPLIB_FILA/scripts/Stage1/scripts/runs/fila-stage1-20250613_172540/checkpoint-500/stage1_projector_cvfm.bin"
IMAGE_PATH = "/root/autodl-tmp/MedPLIB_FILA/scripts/Stage1/test/pexels-photo-32477995.jpeg"
PROMPT = "Describe this image in detail."
DEVICE = "cuda"
# ==============================================================================
# --- è„šæœ¬ä¸»é€»è¾‘ (å¤§éƒ¨åˆ†æ— éœ€ä¿®æ”¹) ---
# ==============================================================================

def monkey_patch_forward(model):
    """
    è¿™æ˜¯ä¸€ä¸ªçŒ´å­è¡¥ä¸å‡½æ•°ã€‚å®ƒä¼šåŠ¨æ€æ›¿æ¢åŸå§‹æ¨¡å‹çš„ forward æ–¹æ³•ã€‚
    """
    # 1. ä¿å­˜å¯¹åŸå§‹ forward æ–¹æ³•çš„å¼•ç”¨
    original_forward = model.forward

    # 2. å®šä¹‰ä¸€ä¸ªæ–°çš„ã€æ›´å®Œå–„çš„ forward æ–¹æ³•
    @wraps(original_forward)
    def patched_forward(**kwargs):
        """
        æ¨ç†é˜¶æ®µå·²åœ¨è„šæœ¬å¤–å®Œæˆå›¾åƒç‰¹å¾æ‹¼æ¥ï¼Œä¹‹ååªéœ€çº¯æ–‡æœ¬è¯­è¨€æ¨¡å‹ã€‚
        å…ˆåˆ é™¤ LlamaForCausalLM ä¸è®¤è¯†çš„å¤šæ¨¡æ€å‚æ•°ï¼Œå†è°ƒç”¨å…¶ forwardã€‚
        """
        # âš ï¸ æ¸…ç†å¤šæ¨¡æ€ç›¸å…³æ®‹ä½™å‚æ•°ï¼Œä¿æŒç­¾åå…¼å®¹
        for invalid in (
            "images", "images_vit", "images_convnext",
            "region_masks", "valid_region_masks_bool"
        ):
            kwargs.pop(invalid, None)
        return LlamaForCausalLM.forward(model, **kwargs)

    # 3. ç”¨æˆ‘ä»¬æ–°å®šä¹‰çš„ forward æ–¹æ³•æ›¿æ¢æ‰æ¨¡å‹å®ä¾‹çš„åŸå§‹æ–¹æ³•
    model.forward = patched_forward
    print("âœ… Model's forward method has been successfully monkey-patched for inference.")
    return model

# load_model, prepare_inputs, å’Œ run_inference å‡½æ•°ä¸ä¸Šä¸€ç‰ˆä¿®å¤ä¸­çš„ä»£ç å®Œå…¨ç›¸åŒ
def load_model(base_llm_path, vision_tower_path, stage1_weights_path, device):
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(base_llm_path, use_fast=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    config = AutoConfig.from_pretrained(base_llm_path)
    config.mm_vision_tower = vision_tower_path
    config.mm_vision_select_layer = -2
    config.mm_vision_select_feature = "patch"
    config.mm_hidden_size = 1024
    config.max_sample_point = 4096
    model = FILAForCausalLM.from_pretrained(
        base_llm_path, config=config, torch_dtype=torch.bfloat16, ignore_mismatched_sizes=True
    ).to(device)
    if tokenizer.add_special_tokens({"additional_special_tokens": [DEFAULT_IMAGE_TOKEN]}) > 0:
        model.resize_token_embeddings(len(tokenizer))
    print(f"ğŸ”§ æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½è®­ç»ƒå¥½çš„æƒé‡: {stage1_weights_path}")
    if not os.path.exists(stage1_weights_path):
        print(f"âŒ é”™è¯¯: æƒé‡æ–‡ä»¶ä¸å­˜åœ¨äº '{stage1_weights_path}'ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        sys.exit(1)
    stage1_weights = torch.load(stage1_weights_path, map_location='cpu')
    model.load_state_dict(stage1_weights, strict=False)
    print("âœ… æƒé‡åŠ è½½æˆåŠŸã€‚")
    model.eval()
    return model, tokenizer

def prepare_inputs(image_path, prompt, tokenizer, device):
    try:
        image = Image.open(image_path).convert("RGB")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨äº '{image_path}'ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        sys.exit(1)
    transform_vit = transforms.Compose([
        transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
    ])
    transform_convnext = transforms.Compose([
        transforms.Resize((768, 768), interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    images_vit = transform_vit(image).unsqueeze(0).to(torch.bfloat16).to(device)
    images_convnext = transform_convnext(image).unsqueeze(0).to(torch.bfloat16).to(device)
    full_prompt = f"{DEFAULT_IMAGE_TOKEN}\n{prompt}"
    input_ids = tokenizer(full_prompt, return_tensors="pt").input_ids.to(device)
    return {"images_vit": images_vit, "images_convnext": images_convnext, "input_ids": input_ids}

def run_inference(model, tokenizer, inputs, prompt):
    print("\nğŸ’¬ æ­£åœ¨ç”Ÿæˆå›åº”...")
    print("-------------------------")
    print(f"æç¤º (PROMPT): {prompt}")
    input_ids = inputs['input_ids']
    images_vit = inputs['images_vit']
    images_convnext = inputs['images_convnext']
    image_features = model.encode_images(images_vit, images_convnext)
    input_embeds = model.get_model().embed_tokens(input_ids)
    image_token_id = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)
    image_token_indices = torch.where(input_ids[0] == image_token_id)[0]
    if len(image_token_indices) == 0:
        print("âŒ é”™è¯¯: åœ¨ prompt ä¸­æ²¡æœ‰æ‰¾åˆ° <image> æ ‡è®°ã€‚")
        return
    image_token_start_index = image_token_indices[0]
    final_inputs_embeds = torch.cat([
        input_embeds[:, :image_token_start_index],
        image_features,
        input_embeds[:, image_token_start_index + 1:]
    ], dim=1)
    attention_mask = torch.ones(final_inputs_embeds.shape[:2], dtype=torch.long, device=DEVICE)
    with torch.inference_mode():
        output_ids = model.generate(
            inputs_embeds=final_inputs_embeds,
            attention_mask=attention_mask,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            use_cache=True
        )
    decoded_output = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
    response = decoded_output.split(prompt)[-1].strip()
    print(f"\næ¨¡å‹å›åº” (MODEL RESPONSE):\n{response}")
    print("-------------------------")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("--- å¼€å§‹æ‰§è¡Œæ¨ç†è„šæœ¬ (å¸¦çŒ´å­è¡¥ä¸ä¿®å¤) ---")
    model, tokenizer = load_model(BASE_LLM_PATH, VISION_TOWER_PATH, STAGE1_WEIGHTS_PATH, DEVICE)
    
    # ğŸ”¥ åœ¨è¿™é‡Œåº”ç”¨çŒ´å­è¡¥ä¸
    model = monkey_patch_forward(model)
    
    inputs = prepare_inputs(IMAGE_PATH, PROMPT, tokenizer, DEVICE)
    run_inference(model, tokenizer, inputs, PROMPT)
    print("--- æ¨ç†ç»“æŸ ---")

if __name__ == "__main__":
    main()