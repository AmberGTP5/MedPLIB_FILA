# file: test_stage2_vqa.py
#
# ç¬¬äºŒé˜¶æ®µVQAä¸“å®¶æ¨¡å‹çš„å®Œæ•´æµ‹è¯•è„šæœ¬ã€‚
# å®ƒä¼šå…ˆåŠ è½½åŸºç¡€æ¨¡å‹å’Œç¬¬ä¸€é˜¶æ®µæƒé‡ï¼Œç„¶åå†åŠ è½½ç¬¬äºŒé˜¶æ®µè®­ç»ƒçš„LoRAé€‚é…å™¨ã€‚

import os
import sys
from pathlib import Path
import torch
from PIL import Image
from torchvision import transforms
from transformers import AutoConfig, AutoTokenizer, logging as transformers_logging
from peft import PeftModel

# --- è·¯å¾„è®¾ç½® ---
# ç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥æˆ‘ä»¬é¡¹ç›®ä¸­çš„æ¨¡å—
PROJECT_ROOT = str(Path(__file__).resolve().parent.parent.parent)
sys.path.insert(0, PROJECT_ROOT)
STAGE1_ROOT = os.path.join(PROJECT_ROOT, "scripts", "Stage1")
sys.path.insert(0, STAGE1_ROOT)

from scripts.Stage1.model.fila_lisa import FILAForCausalLM
from scripts.Stage1.utils.utils import DEFAULT_IMAGE_TOKEN
from scripts.Stage1.model.medplib.model.language_model.medplib_llama import LlamaForCausalLM
from scripts.Stage1.model.medplib import conversation as conversation_lib

transformers_logging.set_verbosity_error()

# ==============================================================================
# --- âš™ï¸ é…ç½®åŒº (HARDCODED CONFIGURATION) ---
# --- è¯·åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„å®é™…è·¯å¾„å’Œè¦æµ‹è¯•çš„å†…å®¹ ---
# ==============================================================================
BASE_LLM_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/Llama-3.1-8B-Instruct"
VISION_TOWER_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/clip-vit-large-patch14-336"

# å…³é”®ï¼šç¬¬ä¸€é˜¶æ®µäº§å‡ºçš„æƒé‡æ–‡ä»¶è·¯å¾„
STAGE1_WEIGHTS_PATH = "/root/autodl-tmp/MedPLIB_FILA/scripts/Stage1/scripts/runs/fila-stage1-20250614_000152/checkpoint-44000/stage1_projector_cvfm.bin"

# â—ï¸â—ï¸â—ï¸ã€æ–°å¢ã€‘ç¬¬äºŒé˜¶æ®µè®­ç»ƒäº§å‡ºçš„LoRAé€‚é…å™¨æ–‡ä»¶å¤¹è·¯å¾„â—ï¸â—ï¸â—ï¸
# è¿™ä¸ªè·¯å¾„åº”è¯¥æŒ‡å‘åŒ…å« adapter_model.bin å’Œ adapter_config.json çš„æ–‡ä»¶å¤¹
STAGE2_LORA_PATH = "/root/autodl-tmp/MedPLIB_FILA/scripts/Stage2/runs/stage2-vqa-deepspeed-20250621_134100"

# æ‚¨æƒ³è¦æµ‹è¯•çš„å›¾ç‰‡
IMAGE_PATH = "/root/autodl-tmp/MedPLIB_FILA/scripts/Stage2/test/ct_00--MSD_Liver--liver_83--y_0343.png"

# æ‚¨æƒ³è¦æå‡ºçš„é—®é¢˜
PROMPT = "What major organs are visible in this CT scan?"

DEVICE = "cuda"
# ==============================================================================
# --- è„šæœ¬ä¸»é€»è¾‘ ---
# ==============================================================================

# å›¾åƒå¤„ç†å™¨ï¼šå°è£…äº†ç¬¬ä¸€é˜¶æ®µä½¿ç”¨çš„å›¾åƒå˜æ¢ï¼Œç¡®ä¿ä¸€è‡´æ€§
class FILAImageProcessor:
    """A callable class to process images for both ViT and ConvNeXt branches."""
    def __init__(self):
        self.vit_transform = transforms.Compose([
            transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
        ])
        self.convnext_transform = transforms.Compose([
            transforms.Resize((768, 768), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    def __call__(self, image):
        return self.vit_transform(image), self.convnext_transform(image)

def load_sft_model(base_llm_path, vision_tower_path, stage1_weights_path, stage2_lora_path, device):
    """
    åŠ è½½å®Œæ•´çš„ã€ç»è¿‡SFTçš„VQAä¸“å®¶æ¨¡å‹ã€‚
    æµç¨‹: åŸºç¡€æ¨¡å‹ -> åŠ è½½Stage1æƒé‡ -> åŠ è½½Stage2 LoRAé€‚é…å™¨
    """
    print("ğŸš€ Initializing base model and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_llm_path, use_fast=False, model_max_length=2048)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    config = AutoConfig.from_pretrained(base_llm_path)
    config.mm_vision_tower = vision_tower_path
    config.mm_vision_select_layer = -2
    config.mm_vision_select_feature = "patch"
    config.mm_hidden_size = 1024
    config.max_sample_point = 4096
    
    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    model = FILAForCausalLM.from_pretrained(
        base_llm_path, config=config, torch_dtype=torch.bfloat16
    ).to(device)
    
    if tokenizer.add_special_tokens({"additional_special_tokens": [DEFAULT_IMAGE_TOKEN]}) > 0:
        model.resize_token_embeddings(len(tokenizer))

    # 2. åŠ è½½ç¬¬ä¸€é˜¶æ®µæƒé‡
    print(f"ğŸ”§ Loading Stage 1 weights from: {stage1_weights_path}")
    if os.path.exists(stage1_weights_path):
        stage1_weights = torch.load(stage1_weights_path, map_location='cpu')
        model.load_state_dict(stage1_weights, strict=False)
        print("âœ… Stage 1 weights loaded successfully.")
    else:
        print(f"âš ï¸ Warning: Stage 1 weights not found at {stage1_weights_path}. Skipping.")

    # 3. ã€æ ¸å¿ƒã€‘åŠ è½½å¹¶åº”ç”¨ç¬¬äºŒé˜¶æ®µçš„LoRAé€‚é…å™¨
    print(f"âœ¨ Loading Stage 2 LoRA adapter from: {stage2_lora_path}")
    model = PeftModel.from_pretrained(model, stage2_lora_path)
    print("âœ… Stage 2 LoRA adapter loaded successfully.")
    
    # ä¸ºäº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥å°†LoRAæƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­
    print("Merging LoRA weights for faster inference...")
    model = model.merge_and_unload()
    
    model.eval()
    return model, tokenizer

def prepare_vqa_inputs(image_path, prompt, tokenizer, device):
    """ä¸ºVQAä»»åŠ¡å‡†å¤‡è¾“å…¥ï¼Œä½¿ç”¨ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„å¯¹è¯æ¨¡æ¿ã€‚"""
    try:
        image = Image.open(image_path).convert("RGB")
    except FileNotFoundError:
        print(f"âŒ Error: Image file not found at '{image_path}'.")
        sys.exit(1)
    
    # ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„å›¾åƒå¤„ç†å™¨
    image_processor = FILAImageProcessor()
    images_vit, images_convnext = image_processor(image)
    images_vit = images_vit.unsqueeze(0).to(torch.bfloat16).to(device)
    images_convnext = images_convnext.unsqueeze(0).to(torch.bfloat16).to(device)
    
    # å¯¹è¯æ¨¡æ¿å¤„ç†
    conv = conversation_lib.conv_templates["llava_v1"].copy()
    question = DEFAULT_IMAGE_TOKEN + '\n' + prompt
    conv.append_message(conv.roles[0], question)
    conv.append_message(conv.roles[1], None) # ç­”æ¡ˆéƒ¨åˆ†ç•™ç©ºï¼Œè®©æ¨¡å‹ç”Ÿæˆ
    
    full_prompt_string = conv.get_prompt()
    input_ids = tokenizer(full_prompt_string, return_tensors="pt").input_ids.to(device)
    
    return {"images_vit": images_vit, "images_convnext": images_convnext, "input_ids": input_ids}


def run_manual_generation(model, tokenizer, inputs, prompt):
    """
    æ‰‹åŠ¨å®ç°çš„ã€å¯é çš„æ–‡æœ¬ç”Ÿæˆå¾ªç¯ã€‚
    """
    print("\nğŸ’¬ Performing manual generation...")
    print("-----------------------------------")
    print(f"PROMPT: {prompt}")

    # --- 1. å‡†å¤‡ç¬¬ä¸€æ¬¡ forward pass çš„è¾“å…¥ ---
    input_ids = inputs['input_ids']
    images_vit = inputs['images_vit']
    images_convnext = inputs['images_convnext']
    
    image_features = model.encode_images(images_vit, images_convnext)
    input_embeds = model.get_model().embed_tokens(input_ids)
    
    image_token_id = tokenizer.convert_tokens_to_ids(DEFAULT_IMAGE_TOKEN)
    image_token_indices = torch.where(input_ids[0] == image_token_id)[0]
    image_token_start_index = image_token_indices[0]
    
    current_embeds = torch.cat([
        input_embeds[:, :image_token_start_index],
        image_features,
        input_embeds[:, image_token_start_index + 1:]
    ], dim=1)
    
    attention_mask = torch.ones(current_embeds.shape[:2], dtype=torch.long, device=DEVICE)
    
    # --- 2. åˆå§‹åŒ–ç”Ÿæˆå¾ªç¯æ‰€éœ€å˜é‡ ---
    max_new_tokens = 512
    generated_ids = []
    past_key_values = None
    
    with torch.inference_mode():
        for i in range(max_new_tokens):
            # --- 3. æ‰§è¡Œå‰å‘ä¼ æ’­ ---
            if i == 0:
                outputs = LlamaForCausalLM.forward(model, inputs_embeds=current_embeds, attention_mask=attention_mask, use_cache=True)
            else:
                outputs = LlamaForCausalLM.forward(model, input_ids=next_token, past_key_values=past_key_values, attention_mask=attention_mask, use_cache=True)

            # --- 4. è·å–ä¸‹ä¸€ä¸ªè¯å…ƒ ---
            logits = outputs.logits
            past_key_values = outputs.past_key_values
            
            next_token_logits = logits[:, -1, :]
            # è¿™é‡Œä½¿ç”¨è´ªå¿ƒè§£ç  (greedy decoding)
            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
            
            # --- 5. ä¿å­˜å¹¶æ£€æŸ¥æ˜¯å¦ç»“æŸ ---
            token_id = next_token.item()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸç¬¦
            if token_id in (tokenizer.eos_token_id, tokenizer.pad_token_id) or (hasattr(tokenizer, 'eot_id') and token_id == tokenizer.eot_id):
                break
                
            generated_ids.append(token_id)
            
            # --- 6. å‡†å¤‡ä¸‹ä¸€æ¬¡å¾ªç¯çš„è¾“å…¥ ---
            attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), dtype=torch.long, device=DEVICE)], dim=1)

    # --- 7. è§£ç å¹¶æ‰“å°ç»“æœ ---
    response = tokenizer.decode(generated_ids, skip_special_tokens=True)
    clean_response = response.strip()
    print(f"\nMODEL RESPONSE:\n{clean_response}")
    print("-----------------------------------")


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("--- Starting Stage 2 VQA Model Test ---")
    model, tokenizer = load_sft_model(BASE_LLM_PATH, VISION_TOWER_PATH, STAGE1_WEIGHTS_PATH, STAGE2_LORA_PATH, DEVICE)
    inputs = prepare_vqa_inputs(IMAGE_PATH, PROMPT, tokenizer, DEVICE)
    run_manual_generation(model, tokenizer, inputs, PROMPT)
    print("--- Inference finished ---")

if __name__ == "__main__":
    main()