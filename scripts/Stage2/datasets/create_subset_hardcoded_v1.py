# file: create_clean_subset.py (FINAL OPTIMIZED VERSION)
#
# æœ€ç»ˆä¼˜åŒ–ç‰ˆï¼šå¢åŠ äº†å¯¹JSONä¸­ç¼ºå°‘.pngåç¼€çš„è·¯å¾„çš„è‡ªåŠ¨å¤„ç†èƒ½åŠ›ã€‚

import json
import os
import shutil
from tqdm import tqdm

# ==============================================================================
# --- âš™ï¸ é…ç½®åŒº (HARDCODED CONFIGURATION) ---
# --- è¯·åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„å®é™…è·¯å¾„å’Œå‚æ•° ---
# ==============================================================================
SOURCE_JSON_PATH = "E:/MeCoVQA/train/MeCoVQA-Region.json"
SOURCE_BASE_DIR = "E:/SAMed2Dv1" 
DEST_JSON_PATH = "D:/SoftData/VSCODE/é¡¶ä¼šè®ºæ–‡/MedPLIB-main/20250607/datasets_subset_2/subset_MeCoVQA_Region_CLEAN.json"
DEST_IMAGES_DIR = "D:/SoftData/VSCODE/é¡¶ä¼šè®ºæ–‡/MedPLIB-main/20250607/datasets_subset_2/subset_images_region_CLEAN/"
NUM_SAMPLES_TO_PROCESS = 10000 
MISSING_FILES_LOG_PATH = "D:/SoftData/VSCODE/é¡¶ä¼šè®ºæ–‡/MedPLIB-main/20250607/datasets_subset_2/missing_files_CLEAN.txt"
# ==============================================================================
# --- è„šæœ¬ä¸»é€»è¾‘ (å·²ä¼˜åŒ–) ---
# ==============================================================================

def create_dataset_subset():
    print("--- Starting Dataset Subset Creation (Optimized Logic for .png extension) ---")
    
    # ... (å‰é¢çš„æ£€æŸ¥å’Œç›®å½•åˆ›å»ºé€»è¾‘ä¿æŒä¸å˜) ...
    os.makedirs(os.path.dirname(DEST_JSON_PATH), exist_ok=True)
    os.makedirs(DEST_IMAGES_DIR, exist_ok=True)
    
    print(f"Reading original annotation file from: {SOURCE_JSON_PATH}")
    with open(SOURCE_JSON_PATH, 'r', encoding='utf-8') as f:
        all_data = json.load(f)

    data_to_process = all_data[:min(NUM_SAMPLES_TO_PROCESS, len(all_data))]
    print(f"Processing first {len(data_to_process)} samples from the source file...")
    
    valid_samples = []
    missing_files_list = []

    for item in tqdm(data_to_process, desc="Validating and copying images"):
        relative_path = item.get('image')
        if not relative_path:
            continue

        # --- æ ¸å¿ƒä¼˜åŒ–åœ¨æ­¤ ---
        source_path = os.path.join(SOURCE_BASE_DIR, relative_path)
        valid_source_path = None

        # 1. é¦–å…ˆï¼Œæ£€æŸ¥åŸå§‹è·¯å¾„æ˜¯å¦å­˜åœ¨
        if os.path.exists(source_path):
            valid_source_path = source_path
        else:
            # 2. å¦‚æœä¸å­˜åœ¨ï¼Œå°è¯•æ·»åŠ  .png åç¼€å†æ£€æŸ¥
            source_path_with_ext = source_path + ".png"
            if os.path.exists(source_path_with_ext):
                valid_source_path = source_path_with_ext

        # --- ä¼˜åŒ–ç»“æŸ ---
        
        if valid_source_path:
            # å¦‚æœæ‰¾åˆ°äº†æœ‰æ•ˆè·¯å¾„ï¼Œåˆ™æ·»åŠ æ ·æœ¬å¹¶å¤åˆ¶å›¾ç‰‡
            valid_samples.append(item)
            
            base_filename = os.path.basename(valid_source_path)
            destination_path = os.path.join(DEST_IMAGES_DIR, base_filename)
            if not os.path.exists(destination_path):
                 shutil.copy2(valid_source_path, destination_path)
        else:
            # å¦‚æœä¸¤ç§æƒ…å†µéƒ½æ‰¾ä¸åˆ°ï¼Œåˆ™è®°å½•ä¸ºç¼ºå¤±
            missing_files_list.append(relative_path)

    print(f"\nSaving {len(valid_samples)} valid samples to: {DEST_JSON_PATH}")
    with open(DEST_JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(valid_samples, f, indent=2)
    
    if missing_files_list:
        print(f"Writing list of {len(missing_files_list)} missing files to: {MISSING_FILES_LOG_PATH}")
        with open(MISSING_FILES_LOG_PATH, 'w', encoding='utf-8') as log_file:
            for missing_path in missing_files_list:
                log_file.write(missing_path + '\n')

    print("\n--- Subset Creation Finished ---")
    print(f"âœ… Total valid samples found and saved: {len(valid_samples)}")
    print(f"âœ… Total images copied: {len(valid_samples)}")
    print(f"âš ï¸ Total samples skipped due to missing images: {len(missing_files_list)}")
    if missing_files_list:
        print(f"ğŸ‘‰ A log of missing files has been saved to: {MISSING_FILES_LOG_PATH}")

if __name__ == "__main__":
    create_dataset_subset()