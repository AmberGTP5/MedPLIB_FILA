# file: scripts/Stage2/train_stage2_vqa_sft.py
#
# ç¬¬äºŒé˜¶æ®µï¼šVQAä¸“å®¶ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰çš„å®Œæ•´è®­ç»ƒè„šæœ¬

import argparse
import os
import sys
from pathlib import Path

import torch
import transformers
from peft import LoraConfig, get_peft_model
from transformers import (AutoConfig, AutoTokenizer, BitsAndBytesConfig, Trainer,
                          TrainingArguments, DataCollatorForSeq2Seq)
from torchvision import transforms

# --- è·¯å¾„è®¾ç½® ---
# å°†é¡¹ç›®æ ¹ç›®å½• (MedPLIB_FILA) æ·»åŠ åˆ° sys.pathï¼Œä»¥ç¡®ä¿å¯ä»¥æ­£ç¡®å¯¼å…¥å…¶ä»–æ¨¡å—
# æ­¤è„šæœ¬ä½äº MedPLIB_FILA/scripts/Stage2/
PROJECT_ROOT = str(Path(__file__).resolve().parent.parent.parent)
sys.path.insert(0, PROJECT_ROOT)

STAGE1_ROOT = os.path.join(PROJECT_ROOT, "scripts", "Stage1")
sys.path.insert(0, STAGE1_ROOT)

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±é¡¹ç›®ä¸­çš„æ¨¡å—
from scripts.Stage1.model.fila_lisa import FILAForCausalLM
from scripts.Stage2.datasets.vqa_dataset import VQADataset

# ----------------------------------------------------------------------------------
# å›¾åƒå¤„ç†å™¨ï¼šå°è£…äº†ç¬¬ä¸€é˜¶æ®µä½¿ç”¨çš„å›¾åƒå˜æ¢ï¼Œç¡®ä¿ä¸€è‡´æ€§
# ----------------------------------------------------------------------------------
class FILAImageProcessor:
    """A callable class to process images for both ViT and ConvNeXt branches."""
    def __init__(self):
        # ViT åˆ†æ”¯çš„å›¾åƒå˜æ¢ (æ¥è‡ª Stage 1)
        self.vit_transform = transforms.Compose([
            transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
        ])
        # ConvNeXt åˆ†æ”¯çš„å›¾åƒå˜æ¢ (æ¥è‡ª Stage 1)
        self.convnext_transform = transforms.Compose([
            transforms.Resize((768, 768), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

    def __call__(self, image):
        # è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ç§å˜æ¢ç»“æœçš„å…ƒç»„
        return self.vit_transform(image), self.convnext_transform(image)

# ----------------------------------------------------------------------------------
# ä¸»è®­ç»ƒå‡½æ•°
# ----------------------------------------------------------------------------------
def train():
    # ==============================================================================
    # --- âš™ï¸ ç¡¬ç¼–ç è·¯å¾„é…ç½®åŒº ---
    # --- è¯·åœ¨æ­¤å¤„å¡«å…¥æ‚¨çš„å®é™…å›ºå®šè·¯å¾„ ---
    # ==============================================================================
    BASE_LLM_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/Llama-3.1-8B-Instruct"
    VISION_TOWER_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/clip-vit-large-patch14-336/"
    STAGE1_WEIGHTS_PATH = "/root/autodl-tmp/MedPLIB_FILA/scripts/Stage1/scripts/runs/fila-stage1-20250614_000152/checkpoint-44000/stage1_projector_cvfm.bin"
    DATA_PATH = "/root/autodl-tmp/MedPLIB_FILA/datasets/datasets_subset_2/subset_MeCoVQA_Region_CLEAN.json"
    IMAGE_FOLDER = "/root/autodl-tmp/MedPLIB_FILA/datasets/datasets_subset_2/subset_images_region_CLEAN/"
    # ==============================================================================

    # --- 1. è§£æå‘½ä»¤è¡Œå‚æ•° (å·²ä¿®æ”¹ï¼Œç§»é™¤äº†è·¯å¾„å‚æ•°) ---
    parser = argparse.ArgumentParser(description="Stage 2: VQA Expert SFT with DeepSpeed")
    
    # -- è®­ç»ƒè¶…å‚æ•°å’Œè¾“å‡ºç›®å½• --
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--num_train_epochs", type=int, default=1)
    parser.add_argument("--learning_rate", type=float, default=2e-4)
    parser.add_argument("--per_device_train_batch_size", type=int, default=2)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=32)
    parser.add_argument("--bf16", type=lambda x: (str(x).lower() == 'true'), default=True)
    parser.add_argument("--logging_steps", type=int, default=10)
    parser.add_argument("--save_strategy", type=str, default="steps")
    parser.add_argument("--save_steps", type=int, default=500)
    parser.add_argument("--save_total_limit", type=int, default=3)
    parser.add_argument("--report_to", type=str, default="tensorboard")
    
    # -- DeepSpeed å…¼å®¹æ€§å‚æ•° --
    parser.add_argument("--local_rank", type=int, default=-1)
    parser.add_argument("--deepspeed", type=str, default=None)

    args = parser.parse_args()

    # --- 2. åˆå§‹åŒ–æ¨¡å‹å’ŒTokenizer ---
    print("ğŸš€ Initializing model and tokenizer for Stage 2 SFT...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_LLM_PATH, use_fast=False, model_max_length=2048) # [ä¿®æ­£]
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    config = AutoConfig.from_pretrained(BASE_LLM_PATH, trust_remote_code=True) # [ä¿®æ­£]
    config.mm_vision_tower = VISION_TOWER_PATH # [ä¿®æ­£]
    config.mm_vision_select_layer = -2
    config.mm_vision_select_feature = "patch"
    config.mm_hidden_size = 1024
    config.max_sample_point = 4096

    model = FILAForCausalLM.from_pretrained(
        BASE_LLM_PATH, # [ä¿®æ­£]
        config=config,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    if tokenizer.add_special_tokens({"additional_special_tokens": ["<image>"]}) > 0:
        model.resize_token_embeddings(len(tokenizer))
    
    model.get_model().vision_tower.vision_tower.load_model()
    model.get_model().vision_tower.vision_tower.tokenizer = tokenizer

    # --- 3. åŠ è½½ç¬¬ä¸€é˜¶æ®µæƒé‡å¹¶åº”ç”¨LoRA ---
    print(f"ğŸ”§ Loading Stage 1 weights from: {STAGE1_WEIGHTS_PATH}")
    stage1_weights = torch.load(STAGE1_WEIGHTS_PATH, map_location='cpu') # [ä¿®æ­£]
    model.load_state_dict(stage1_weights, strict=False)
    print("âœ… Stage 1 weights loaded successfully.")
    
    print("ğŸ§Š Freezing original parameters and applying LoRA...")
    for param in model.parameters():
        param.requires_grad = False
        if param.ndim == 1:
            param.data = param.data.to(torch.float32)

    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()
    
    lora_config = LoraConfig(
        r=128,
        lora_alpha=256,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # --- 4. å‡†å¤‡æ•°æ®é›† ---
    print("ğŸ“š Preparing VQA dataset...")
    image_processor = FILAImageProcessor()
    train_dataset = VQADataset(
        data_path=DATA_PATH, # [ä¿®æ­£]
        image_folder=IMAGE_FOLDER, # [ä¿®æ­£]
        tokenizer=tokenizer,
        image_processor=image_processor
    )
    
    # --- 5. è®¾ç½®å¹¶å¯åŠ¨è®­ç»ƒ ---
    print("ğŸ”¥ Setting up Trainer and starting SFT...")
    
    # â—ï¸â—ï¸â—ï¸æ³¨æ„ï¼šè¿™é‡Œçš„å˜é‡åéœ€è¦ä¸å¯åŠ¨è„šæœ¬ä¸­çš„å‘½ä»¤è¡Œå‚æ•°åå®Œå…¨å¯¹åº”â—ï¸â—ï¸â—ï¸
    print("ğŸ”¥ Setting up Trainer and starting SFT...")
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        bf16=args.bf16,
        logging_steps=args.logging_steps,
        save_strategy=args.save_strategy,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        report_to=args.report_to,
        remove_unused_columns=False,
        deepspeed=args.deepspeed,
    )
    
    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        pad_to_multiple_of=8,
        return_tensors="pt",
        padding=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )

    trainer.train()

    trainer.save_model(args.output_dir)
    print(f"ğŸ‰ Stage 2 SFT finished. Final LoRA adapter saved in {args.output_dir}")


if __name__ == "__main__":
    train()