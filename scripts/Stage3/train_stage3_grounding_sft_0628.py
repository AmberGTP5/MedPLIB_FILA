# file: scripts/Stage3/train_stage3_grounding_sft.py
#
# æœ€ç»ˆçš„ã€ç»Ÿä¸€çš„ã€è‡ªç»™è‡ªè¶³çš„ç¬¬ä¸‰é˜¶æ®µè®­ç»ƒè„šæœ¬
# é›†æˆäº†æ‰€æœ‰ä¾èµ–ç±»å’Œç»è¿‡éªŒè¯çš„åˆå§‹åŒ–é€»è¾‘

import argparse
import os
import sys
import json
import re
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset

import transformers
from transformers import (AutoConfig, AutoTokenizer, Trainer,
                          TrainingArguments, default_data_collator)
from transformers.modeling_outputs import CausalLMOutputWithPast
from torchvision import transforms
from transformers import default_data_collator

# --- è·¯å¾„è®¾ç½® ---
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°æœç´¢è·¯å¾„ï¼Œä»¥å¯¼å…¥Stage1çš„æ¨¡å‹
PROJECT_ROOT = str(Path(__file__).resolve().parent.parent.parent)
sys.path.insert(0, PROJECT_ROOT)

from datasets.grounding_dataset import GroundingDataset # å¯¼å…¥ä¿®æ”¹åçš„æ•°æ®é›†ç±»
from model.fila_lisa_for_stage3 import FILAForCausalLM as FILABaseModel
# å¯¼å…¥MedPLIBä¸­çš„SAMæ¨¡å‹æ„é€ å™¨ï¼Œå‡è®¾å®ƒåœ¨æˆ‘ä»¬çš„Stage3/modelç›®å½•ä¸‹
from model.segment_anything_med2d import build_sam_vit_b

# ==============================================================================
# --- ç¬¬1éƒ¨åˆ†: æ‰€æœ‰å¿…éœ€çš„ç±»å®šä¹‰ ---
# ==============================================================================

# --- å›¾åƒå¤„ç†å™¨ ---
class FILAImageProcessor:
    """A callable class to process images for both ViT and ConvNeXt branches."""
    def __init__(self):
        self.vit_transform = transforms.Compose([
            transforms.Resize((336, 336), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
        ])
        self.convnext_transform = transforms.Compose([
            transforms.Resize((768, 768), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    def __call__(self, image):
        return self.vit_transform(image), self.convnext_transform(image)

# --- æ¨¡å‹å®šä¹‰ ---
def dice_loss(inputs, targets, eps=1e-6):
    inputs = torch.sigmoid(inputs)
    inputs = inputs.flatten(1)
    targets = targets.flatten(1)
    intersection = (inputs * targets).sum(1)
    union = inputs.sum(1) + targets.sum(1)
    dice_score = (2. * intersection + eps) / (union + eps)
    return 1 - dice_score.mean()

class UniMedVLMForGrounding(FILABaseModel):
    def __init__(self, config, sam_pretrained_path=None, seg_token_idx=0):
        super().__init__(config)
        
        self.seg_token_idx = seg_token_idx
        
        print("Initializing SAM-Med2D sub-module...")
        sam_model = build_sam_vit_b(sam_checkpoint=sam_pretrained_path)
        self.visual_model = sam_model # é›†æˆæ•´ä¸ªSAMæ¨¡å‹
        
        llm_hidden_size = config.hidden_size
        sam_feature_dim = 256
        self.text_hidden_fcs = nn.Linear(llm_hidden_size, sam_feature_dim)
        
    def forward(self, images_vit=None, images_convnext=None, sam_images=None, input_ids=None, labels=None, gt_masks=None, **kwargs):
        if self.training: # åªåœ¨è®­ç»ƒæ—¶æ‰“å°ï¼Œé¿å…æ¨ç†æ—¶ä¹Ÿè¾“å‡º
            print("\n" + "="*20 + " SHAPE CHECK 5 (Inside Forward Call) " + "="*20)
            print(f"Shape of pos_embed right before use: {self.visual_model.image_encoder.pos_embed.shape}")
            print("="*70 + "\n")
        # ---------------------------

        outputs = super().forward(
            images_vit=images_vit, images_convnext=images_convnext,
            input_ids=input_ids, labels=labels,
            output_hidden_states=True, return_dict=True
        )
        
        lm_loss = outputs.loss if outputs.loss is not None else torch.tensor(0.0, device=input_ids.device)
        
        seg_loss = torch.tensor(0.0, device=input_ids.device)

        if gt_masks is not None:
            last_hidden_state = outputs.hidden_states[-1]
            seg_token_mask = (input_ids == self.seg_token_idx)
            if torch.sum(seg_token_mask) > 0:
                seg_token_features = last_hidden_state[seg_token_mask]
                prompt_embeddings = self.text_hidden_fcs(seg_token_features)
                with torch.no_grad():
                    # [æœ€ç»ˆæ ¸å¿ƒä¿®æ­£] å¿…é¡»å°†ä¸ºSAMå‡†å¤‡çš„sam_imagesä¼ å…¥SAMè‡ªå·±çš„ç¼–ç å™¨ï¼
                    image_embeddings = self.visual_model.image_encoder(sam_images)

                # [æ ¸å¿ƒä¿®æ­£] ä¿®æ­£å¯¹CLIPè§†è§‰æ¨¡å‹çš„è°ƒç”¨æ–¹å¼
                # # 1. é¦–å…ˆæ‹¿åˆ° CLIPVisionModel å¯¹è±¡
                # clip_vision_model = self.get_model().vision_tower.vision_tower.vision_tower
                # # 2. ç›´æ¥è°ƒç”¨å®ƒæ¥ç¼–ç å›¾åƒï¼Œå¹¶è·å–å…¶ last_hidden_state
                # image_embeddings = self.visual_model.image_encoder(sam_images)

                low_res_masks, iou_predictions = self.visual_model.mask_decoder(
                    image_embeddings=image_embeddings,
                    image_pe=self.visual_model.prompt_encoder.get_dense_pe(),
                    sparse_prompt_embeddings=prompt_embeddings.unsqueeze(1),
                    dense_prompt_embeddings=self.visual_model.prompt_encoder.get_dense_pe(),
                    multimask_output=False,
                )
                
                predicted_mask_upsampled = F.interpolate(
                    low_res_masks, size=gt_masks.shape[-2:], mode='bilinear', align_corners=False
                )
                
                bce = F.binary_cross_entropy_with_logits(predicted_mask_upsampled.squeeze(1), gt_masks)
                dice = dice_loss(predicted_mask_upsampled.squeeze(1), gt_masks)
                seg_loss = bce + dice

        total_loss = lm_loss + seg_loss 
        return CausalLMOutputWithPast(loss=total_loss, logits=outputs.logits)

# --- ä¸»è®­ç»ƒå‡½æ•° ---
def main():
    # --- âš™ï¸ ç¡¬ç¼–ç è·¯å¾„é…ç½®åŒº ---
    # ==============================================================================
    BASE_LLM_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/Llama-3.1-8B-Instruct"
    VISION_TOWER_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/clip-vit-large-patch14-336/"
    STAGE1_WEIGHTS_PATH = "/root/autodl-tmp/MedPLIB_FILA/scripts/Stage1/scripts/runs/fila-stage1-20250614_000152/checkpoint-44000/stage1_projector_cvfm.bin"
    SAM_PRETRAINED_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/SA-Med2D/sam-med2d_b.pth"
    DATA_PATH = "/root/autodl-tmp/MedPLIB_FILA/datasets/datasets_subset_3/subset_MeCoVQA_Grounding_CLEAN.json"
    DATA_BASE_DIR = "/root/autodl-tmp/MedPLIB_FILA/datasets/datasets_subset_3/"
    # ==============================================================================
    
    # --- 1. è§£æå‘½ä»¤è¡Œå‚æ•° ---
    parser = argparse.ArgumentParser(description="Stage 3: Grounding Expert SFT")
    parser.add_argument("--output_dir", type=str, required=True)
    parser.add_argument("--sft_modules", type=str, default="mask_decoder,text_hidden_fcs")
    parser.add_argument("--num_train_epochs", type=int, default=3)
    parser.add_argument("--max_steps", type=int, default=-1, help="If > 0: overrides num_train_epochs.")
    parser.add_argument("--learning_rate", type=float, default=5e-5)
    parser.add_argument("--per_device_train_batch_size", type=int, default=1)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=16)
    parser.add_argument("--bf16", type=lambda x: (str(x).lower() == 'true'), default=True)
    parser.add_argument("--logging_steps", type=int, default=10)
    parser.add_argument("--save_strategy", type=str, default="steps")
    parser.add_argument("--save_steps", type=int, default=500)
    parser.add_argument("--save_total_limit", type=int, default=3)
    parser.add_argument("--report_to", type=str, default="tensorboard")
    parser.add_argument("--local_rank", type=int, default=-1)
    parser.add_argument("--deepspeed", type=str, default=None)
    parser.add_argument("--version", type=str, default=BASE_LLM_PATH)
    parser.add_argument("--vision_tower", type=str, default=VISION_TOWER_PATH)
    args = parser.parse_args()

    # --- åˆå§‹åŒ–æ¨¡å‹å’ŒTokenizer ---
    # print("ğŸš€ Initializing model and tokenizer for Stage 3 Grounding SFT...")
    # tokenizer = AutoTokenizer.from_pretrained(BASE_LLM_PATH, use_fast=False, local_files_only=True)
    # if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    # tokenizer.add_tokens(["<SEG>"], special_tokens=True)
    # seg_token_idx = tokenizer.convert_tokens_to_ids("<SEG>")

    # [æ ¸å¿ƒä¿®æ­£] é‡‡ç”¨ä¸test_dataset.pyä¸­éªŒè¯é€šè¿‡çš„ã€å®Œå…¨ç›¸åŒçš„åˆå§‹åŒ–æ–¹å¼
    print("ğŸš€ Initializing model and tokenizer for Stage 3 Grounding SFT...")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_LLM_PATH,
        use_fast=False,         # â—ï¸ç¡®ä¿ä¸ºFalse
        local_files_only=True,  # ç¡®ä¿ç¦»çº¿
        add_bos_token=False,    # â—ï¸ç¡®ä¿ä¸ºFalse
        add_eos_token=False     # â—ï¸ç¡®ä¿ä¸ºFalse
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    tokenizer.add_tokens(["<SEG>"], special_tokens=True)
    # seg_token_idx çš„è·å–ä¹Ÿç§»åˆ°è¿™é‡Œ
    seg_token_idx = tokenizer.convert_tokens_to_ids("<SEG>")
    
    print("âœ… Tokenizer initialized with correct settings.")

    # Register the <image> token
    special_tokens_dict = {'additional_special_tokens': ['<image>']}
    tokenizer.add_special_tokens(special_tokens_dict)
    model.resize_token_embeddings(len(tokenizer))
        
    # --- [æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼šæ‰‹åŠ¨ã€åˆ†æ­¥ã€å¯æ§åœ°æ„å»ºæ¨¡å‹] ---
    print("Step 1: Creating a complete, custom 'config' object...")
    config = AutoConfig.from_pretrained(BASE_LLM_PATH, local_files_only=True)
    config.mm_vision_tower = VISION_TOWER_PATH
    config.vision_tower = VISION_TOWER_PATH
    config.mm_vision_select_layer = -2
    config.mm_vision_select_feature = "patch"
    config.mm_hidden_size = 1024
    config.train_mask_decoder = True
    config.out_dim = 256
    config.max_sample_point = 4096 # <-- è¡¥ä¸Šè¿™ä¸ªç¼ºå¤±çš„å±æ€§
    print("âœ… Config object created and fully populated.")
    
    print("Step 2: Instantiating our custom UniMedVLMForGrounding model framework...")
    model = UniMedVLMForGrounding(config, sam_pretrained_path=SAM_PRETRAINED_PATH, seg_token_idx=seg_token_idx)
    
    # ç›‘æ§ç‚¹ 1: æ¨¡å‹åˆšåˆšåˆå§‹åŒ–å
    print("\n" + "="*20 + " SHAPE CHECK 1 (After Model Init) " + "="*20)
    print(f"Shape of pos_embed: {model.visual_model.image_encoder.pos_embed.shape}")
    print("="*68 + "\n")

    # ä¿®å¤ç‚¹ A: åŠ¨æ€è°ƒæ•´SAMçš„ä½ç½®ç¼–ç 
    print("ğŸ§Š Dynamically adjusting SAM's positional embedding for 1024x1024 input...")
    target_embedding_size = (64, 64)
    original_pos_embed = model.visual_model.image_encoder.pos_embed
    if original_pos_embed.shape[1:3] != target_embedding_size:
        original_pos_embed_transposed = original_pos_embed.permute(0, 3, 1, 2)
        new_pos_embed_transposed = F.interpolate(
            original_pos_embed_transposed, size=target_embedding_size, mode='bilinear', align_corners=False
        )
        new_pos_embed = new_pos_embed_transposed.permute(0, 2, 3, 1)
        model.visual_model.image_encoder.pos_embed = nn.Parameter(new_pos_embed)
        print("âœ… SAM's positional embedding resized successfully.")
    else:
        print("âœ… SAM's positional embedding already has the correct size.")
    
    # ç›‘æ§ç‚¹ 2: æ‰‹åŠ¨ä¿®æ­£ pos_embed ä¹‹å
    print("\n" + "="*20 + " SHAPE CHECK 2 (After pos_embed Fix) " + "="*20)
    print(f"Shape of pos_embed: {model.visual_model.image_encoder.pos_embed.shape}")
    print("="*68 + "\n")

    # ä¿®å¤ç‚¹ B: åŠ¨æ€è°ƒæ•´SAMçš„Mask Decoder
    print("ğŸ§Š Dynamically adjusting SAM's Mask Decoder output upsampling...")
    mask_decoder = model.visual_model.mask_decoder
    mask_decoder.output_upscaling = nn.Sequential(
        nn.ConvTranspose2d(256, 256 // 4, kernel_size=2, stride=2),
        nn.LayerNorm(256 // 4),
        nn.GELU(),
        nn.ConvTranspose2d(256 // 4, 256 // 8, kernel_size=2, stride=2),
        nn.GELU(),
    )
    mask_decoder.output_upscaling.to(device=model.device, dtype=model.dtype)
    print("âœ… SAM's Mask Decoder upscaling layer re-initialized.")

    # ä¿®å¤ç‚¹ C: å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–mm_projector
    print("ğŸ§Š Re-initializing mm_projector to ensure correct dimensions [1024 -> 4096]...")
    mm_hidden_size = model.config.mm_hidden_size
    hidden_size = model.config.hidden_size
    model.model.mm_projector = nn.Sequential(
        nn.Linear(mm_hidden_size, hidden_size),
        nn.GELU(),
        nn.Linear(hidden_size, hidden_size)
    )
    model.model.mm_projector.to(device=model.device, dtype=model.dtype)
    print(f"âœ… mm_projector re-initialized successfully.")

    print("Step 3: Loading base LLM weights...")
    model.load_state_dict(
        transformers.LlamaForCausalLM.from_pretrained(BASE_LLM_PATH, torch_dtype=torch.bfloat16, local_files_only=True).state_dict(),
        strict=False
    )
    
    # ç›‘æ§ç‚¹ 3: åŠ è½½å®ŒLLMæƒé‡ä¹‹å
    print("\n" + "="*20 + " SHAPE CHECK 3 (After LLM Load) " + "="*20)
    print(f"Shape of pos_embed: {model.visual_model.image_encoder.pos_embed.shape}")
    print("="*68 + "\n")

    print(f"Step 4: Loading Stage 1 weights from: {STAGE1_WEIGHTS_PATH}")
    model.load_state_dict(torch.load(STAGE1_WEIGHTS_PATH, map_location='cpu'), strict=False)

    # ç›‘æ§ç‚¹ 4: åŠ è½½å®ŒStage1æƒé‡ä¹‹å
    print("\n" + "="*20 + " SHAPE CHECK 4 (After Stage1 Load) " + "="*20)
    print(f"Shape of pos_embed: {model.visual_model.image_encoder.pos_embed.shape}")
    print("="*68 + "\n")
    
    # æ‚¨å·²ç»æ³¨é‡Šæ‰äº† resize_token_embeddingsï¼Œè¿™æ˜¯æ­£ç¡®çš„
    # model.resize_token_embeddings(len(tokenizer))
    # ä¸ºç¡®ä¿ä¸‡æ— ä¸€å¤±ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰‹åŠ¨ç‰ˆæœ¬
    print("ğŸ§Š Manually resizing token embeddings and output layer for new tokens...")
    new_num_tokens = len(tokenizer)
    input_embeddings = model.get_model().embed_tokens
    old_num_tokens, old_embedding_dim = input_embeddings.weight.shape
    if old_num_tokens != new_num_tokens:
        new_input_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim, device=model.device, dtype=model.dtype)
        new_input_embeddings.weight.data[:old_num_tokens, :] = input_embeddings.weight.data.clone()
        avg_embedding = input_embeddings.weight.data.mean(dim=0)
        new_input_embeddings.weight.data[old_num_tokens:, :] = avg_embedding
        model.get_model().embed_tokens = new_input_embeddings
    output_embeddings = model.lm_head
    if output_embeddings.weight.shape[0] != new_num_tokens:
        new_lm_head = nn.Linear(output_embeddings.in_features, new_num_tokens, bias=False, device=model.device, dtype=model.dtype)
        new_lm_head.weight.data[:old_num_tokens, :] = output_embeddings.weight.data.clone()
        if 'avg_embedding' in locals():
            new_lm_head.weight.data[old_num_tokens:, :] = avg_embedding
        model.lm_head = new_lm_head
    model.config.vocab_size = new_num_tokens
    print(f"âœ… Final embeddings and lm_head resized from {old_num_tokens} to {new_num_tokens}.")

    print("âœ… All components assembled and weights loaded successfully.")
    
    # --- ç²¾ç¡®å†»ç»“/è§£å†»å‚æ•° ---
    print("ğŸ§Š Freezing parameters for Stage 3 SFT...")
    for param in model.parameters(): param.requires_grad = False
    
    trainable_modules = args.sft_modules.split(',')
    for name, param in model.named_parameters():
        for tm in trainable_modules:
            if tm in name:
                param.requires_grad = True
                break
    
    # --- [æ ¸å¿ƒä¿®æ­£] ---
    # æ‰‹åŠ¨è®¡ç®—å¹¶æ‰“å°å¯è®­ç»ƒå‚æ•°
    print("Calculating trainable parameters...")
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"  - Trainable params: {trainable_params:,}")
    print(f"  - All params: {total_params:,}")
    print(f"  - Trainable %: {100 * trainable_params / total_params:.4f}%")
    
    # --- å‡†å¤‡æ•°æ®é›† ---
    image_processor = FILAImageProcessor()
    train_dataset = GroundingDataset(
        data_path=DATA_PATH, data_base_dir=DATA_BASE_DIR,
        tokenizer=tokenizer, image_processor=image_processor
    )

    # --- [æ–°å¢è°ƒè¯•ä¿¡æ¯] ---
    print("\n" + "="*30)
    print("--- [DEBUG] COLLATOR PREPARATION ---")
    
    # æ­¥éª¤ 1: åˆ›å»º DataCollatorForSeq2Seq å®ä¾‹
    print("[DEBUG] Step 1: Creating DataCollatorForSeq2Seq instance...")
    data_collator = default_data_collator
    # æ­¥éª¤ 2: æ‰“å°å³å°†ä¼ å…¥Trainerçš„collatorçš„ç±»å‹
    print(f"[DEBUG] Step 2: The object being passed to Trainer as 'data_collator' is of type: {type(data_collator)}")
    print("="*30 + "\n")
    # --- è°ƒè¯•ä¿¡æ¯ç»“æŸ ---


    # --- è®¾ç½®å¹¶å¯åŠ¨è®­ç»ƒ ---
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        learning_rate=args.learning_rate,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        bf16=args.bf16,
        logging_steps=args.logging_steps,
        save_strategy=args.save_strategy,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,
        report_to=args.report_to,
        remove_unused_columns=False,
        deepspeed=args.deepspeed,
        # å¦‚æœæ‚¨çš„ .sh è„šæœ¬ä¸­ä¹Ÿä¼ å…¥äº† max_steps, è¿™é‡Œä¹Ÿéœ€è¦åŠ ä¸Š
        max_steps=args.max_steps if args.max_steps > 0 else -1,
    )
    print("ğŸ”¥ Starting SFT...")

    trainer = Trainer(
        model=model, args=training_args, train_dataset=train_dataset, data_collator=data_collator
    )

    # ç„¶åå†è°ƒç”¨ .train() æ–¹æ³•
    trainer.train()

    # --- ä¿å­˜æ¨¡å‹ ---
    state_dict_to_save = {k: v for k, v in model.state_dict().items() if v.requires_grad}
    torch.save(state_dict_to_save, os.path.join(args.output_dir, "stage3_grounding_weights.bin"))
    print(f"ğŸ‰ Stage 3 SFT finished. Grounding expert weights saved in {args.output_dir}")

if __name__ == "__main__":
    main()