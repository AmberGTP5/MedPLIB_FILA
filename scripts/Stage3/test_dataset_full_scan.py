# file: test_dataset_full_scan.py (Corrected Version)
#
# ä¸€ä¸ªç”¨äºå¯¹æ•´ä¸ªGroundingDatasetè¿›è¡Œå…¨é¢å¥åº·æ£€æŸ¥çš„è„šæœ¬
# ä½¿ç”¨ç»è¿‡éªŒè¯çš„ã€æ­£ç¡®çš„è·¯å¾„é…ç½®

import os
import sys
from pathlib import Path
from torchvision import transforms
import torch
from transformers import AutoTokenizer
from tqdm import tqdm

# --- è·¯å¾„è®¾ç½® ---
# è¿™ä¸ªè®¾ç½®æ˜¯ä¸ºäº†è®©æ­¤è„šæœ¬èƒ½æ‰¾åˆ°åŒçº§ç›®å½•ä¸‹çš„ train_stage3_grounding_sft.py
# å¹¶ä»ä¸­å¯¼å…¥ GroundingDataset å’Œ FILAImageProcessor
# [ä¿æŒä¸å˜]
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# ä¸ºäº†èƒ½å¯¼å…¥æ­£ç¡®çš„GroundingDatasetï¼Œéœ€è¦å°†Stage3ç›®å½•åŠ å…¥sys.path
# train_stage3_grounding_sft.pyå°±åœ¨å½“å‰ç›®å½•ï¼Œæ‰€ä»¥æˆ‘ä»¬ç›´æ¥ä»å®ƒå¯¼å…¥
sys.path.insert(0, os.path.dirname(SCRIPT_DIR)) # æ·»åŠ  'scripts' ç›®å½•
from Stage3.train_stage3_grounding_sft import GroundingDataset, FILAImageProcessor

def full_dataset_scan():
    """
    å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œä¸€æ¬¡å®Œæ•´çš„æ‰«æï¼Œæ‰¾å‡ºæ‰€æœ‰å¤„ç†å¤±è´¥çš„æ ·æœ¬ã€‚
    """
    print("--- [Dataset Full Scan] Starting a full health check of the dataset... ---")

    # --- 1. [æ ¸å¿ƒä¿®æ­£] ä½¿ç”¨ä¸æ‚¨éªŒè¯é€šè¿‡çš„test_dataset.pyå®Œå…¨ç›¸åŒçš„è·¯å¾„é…ç½® ---
    DATA_PATH = "/root/autodl-tmp/MedPLIB_FILA/datasets/datasets_subset_3/subset_MeCoVQA_Grounding_CLEAN.json"
    DATA_BASE_DIR = "/root/autodl-tmp/MedPLIB_FILA/datasets/datasets_subset_3/"
    BASE_LLM_PATH = "/root/autodl-tmp/MedPLIB_FILA/models/Llama-3.1-8B-Instruct"

    print(f"  - Using Annotation File: {DATA_PATH}")
    print(f"  - Using Base Directory for Images/Masks: {DATA_BASE_DIR}")

    # --- 2. åˆå§‹åŒ– Tokenizer å’Œ Image Processor ---
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_LLM_PATH,
        use_fast=False,
        local_files_only=True,
        add_bos_token=False,
        add_eos_token=False
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.add_tokens(["<SEG>"], special_tokens=True)
    image_processor = FILAImageProcessor()
    print("âœ… Tokenizer and Image Processor initialized.")

    # --- 3. åˆ›å»ºæ•°æ®é›†å®ä¾‹ ---
    # æˆ‘ä»¬éœ€è¦åœ¨GroundingDatasetçš„__getitem__ä¸­ä½¿ç”¨ä¸Šæ¬¡ä¿®æ”¹çš„ã€èƒ½æš´éœ²çœŸå®é”™è¯¯çš„ç‰ˆæœ¬
    dataset = GroundingDataset(
        data_path=DATA_PATH,
        data_base_dir=DATA_BASE_DIR,
        tokenizer=tokenizer,
        image_processor=image_processor,
    )
    print(f"âœ… Dataset instance created. Total samples to check: {len(dataset)}")

    # --- 4. éå†æ£€æŸ¥æ‰€æœ‰æ ·æœ¬ ---
    bad_samples = []
    for i in tqdm(range(len(dataset)), desc="Scanning samples"):
        try:
            # å°è¯•è·å–å¹¶å¤„ç†æ ·æœ¬
            sample = dataset[i]
            if sample is None:
                # è®°å½•ç”±æ˜¾å¼ `return None` å¯¼è‡´çš„åæ ·æœ¬
                bad_samples.append({'index': i, 'error': 'Dataset class explicitly returned None (likely no <mask> tag).'})
        except Exception as e:
            # è®°å½•ç”±å…¶ä»–å¼‚å¸¸ï¼ˆå¦‚FileNotFoundï¼‰å¯¼è‡´çš„åæ ·æœ¬
            bad_samples.append({'index': i, 'error': f"Exception: {type(e).__name__} - {e}"})

    # --- 5. æ‰“å°æœ€ç»ˆçš„â€œä½“æ£€æŠ¥å‘Šâ€ ---
    print("\n--- [Full Scan Report] ---")
    if not bad_samples:
        print("ğŸ‰ Congratulations! All 12380 samples in the dataset were processed successfully!")
    else:
        print(f"ğŸš¨ Found {len(bad_samples)} problematic samples out of {len(dataset)}.")
        print("--- List of Bad Samples (showing first 20) ---")
        for bad_sample in bad_samples[:20]: # åªæ‰“å°å‰20ä¸ªï¼Œé¿å…åˆ·å±
            print(f"  - Index: {bad_sample['index']}, Error: {bad_sample['error']}")
        if len(bad_samples) > 20:
            print(f"  ... and {len(bad_samples) - 20} more.")
        print("\n[ACTION REQUIRED] Please review the list above to clean your dataset.")

    print("--- Scan Finished ---")

if __name__ == "__main__":
    full_dataset_scan()