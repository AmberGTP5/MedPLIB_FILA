# file: stage1_merge_datasets.py

import json
import os
import random
import argparse
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict

def load_json_data(json_path):
    """
    å®‰å…¨åœ°è¯»å–JSON/JSONLæ–‡ä»¶ï¼Œæ”¯æŒä¸¤ç§æ ¼å¼
    """
    json_path = Path(json_path)
    if not json_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
    
    data = []
    try:
        # é¦–å…ˆå°è¯•ä½œä¸ºæ ‡å‡†JSONæ–‡ä»¶è¯»å–
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"âœ… æˆåŠŸè¯»å–JSONæ–‡ä»¶: {json_path.name}")
    except json.JSONDecodeError:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä½œä¸ºJSONLæ–‡ä»¶è¯»å–
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = [json.loads(line.strip()) for line in f if line.strip()]
            print(f"âœ… æˆåŠŸè¯»å–JSONLæ–‡ä»¶: {json_path.name}")
        except json.JSONDecodeError as e:
            raise ValueError(f"æ— æ³•è§£æJSON/JSONLæ–‡ä»¶ {json_path}: {e}")
    except Exception as e:
        raise RuntimeError(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ {json_path}: {e}")
    
    return data

def process_and_normalize_data(json_path, image_base_dir, desc="Processing", keep_full_path=True):
    """
    è¯»å–JSONæ–‡ä»¶ï¼Œæå–imageå’Œcaptionï¼Œå¹¶å¤„ç†å›¾ç‰‡è·¯å¾„ã€‚
    
    Args:
        json_path: JSONæ–‡ä»¶è·¯å¾„
        image_base_dir: å›¾ç‰‡åŸºç¡€ç›®å½•
        desc: è¿›åº¦æ¡æè¿°
        keep_full_path: æ˜¯å¦ä¿ç•™å®Œæ•´è·¯å¾„ï¼ˆæ¨èä¸ºTrueï¼‰
    """
    data = load_json_data(json_path)
    processed_data = []
    skipped_count = 0
    filename_counts = defaultdict(int)
    
    for item in tqdm(data, desc=desc):
        try:
            caption = ""
            image_path = ""

            # æå–å›¾ç‰‡è·¯å¾„
            if 'image' in item:
                original_path = item['image']
                if keep_full_path:
                    # ä¿ç•™ç›¸å¯¹è·¯å¾„ç»“æ„ï¼Œä¾¿äºè®­ç»ƒæ—¶åŠ è½½
                    image_path = str(Path(original_path).as_posix())
                else:
                    # åªä¿ç•™æ–‡ä»¶åï¼ˆåŸå§‹é€»è¾‘ï¼‰
                    image_path = Path(original_path).name
                
                # ç»Ÿè®¡æ–‡ä»¶åé‡å¤æƒ…å†µ
                filename_counts[Path(original_path).name] += 1
            
            # æå–caption
            if 'conversations' in item and len(item['conversations']) >= 2:
                # LLaVAæ ¼å¼çš„å¯¹è¯æ•°æ®
                for conv in item['conversations']:
                    if conv.get('from') in ['gpt', 'assistant']:
                        caption = conv.get('value', '').strip()
                        break
            elif 'caption' in item:
                # ç›´æ¥çš„captionå­—æ®µ
                caption = str(item['caption']).strip()
            
            # æ•°æ®éªŒè¯
            if not image_path or not caption:
                skipped_count += 1
                continue
            
            # æ£€æŸ¥captionè´¨é‡
            if len(caption) < 5:  # è¿‡çŸ­çš„captionå¯èƒ½è´¨é‡ä¸é«˜
                skipped_count += 1
                continue
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¸ºcaptionæ·»åŠ <image>æ ‡ç­¾ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
            if not caption.startswith('<image>'):
                caption = '<image>\n' + caption
            
            processed_data.append({
                "id": item.get('id', f"{Path(json_path).stem}_{len(processed_data)}"),
                "image": image_path,
                "caption": caption,
                "source": Path(json_path).stem  # æ·»åŠ æ•°æ®æºæ ‡è¯†
            })
            
        except Exception as e:
            print(f"âš ï¸  å¤„ç†æ•°æ®é¡¹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            skipped_count += 1
            continue
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š {desc} ç»Ÿè®¡:")
    print(f"   - åŸå§‹æ•°æ®: {len(data)} æ¡")
    print(f"   - æœ‰æ•ˆæ•°æ®: {len(processed_data)} æ¡")
    print(f"   - è·³è¿‡æ•°æ®: {skipped_count} æ¡")
    print(f"   - å·²ä¸ºæ‰€æœ‰captionæ·»åŠ <image>æ ‡ç­¾")
    
    # æ£€æŸ¥æ–‡ä»¶åé‡å¤æƒ…å†µ
    duplicates = {name: count for name, count in filename_counts.items() if count > 1}
    if duplicates and not keep_full_path:
        print(f"âš ï¸  å‘ç°é‡å¤æ–‡ä»¶å: {len(duplicates)} ä¸ª")
        print("   å»ºè®®ä½¿ç”¨ keep_full_path=True é¿å…è·¯å¾„å†²çª")
    
    return processed_data

def validate_and_merge_datasets(datasets_info, output_path, image_base_dirs=None, keep_full_path=True):
    """
    éªŒè¯å¹¶åˆå¹¶å¤šä¸ªæ•°æ®é›†
    
    Args:
        datasets_info: [(json_path, description), ...] æ•°æ®é›†ä¿¡æ¯åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        image_base_dirs: å¯¹åº”çš„å›¾ç‰‡åŸºç¡€ç›®å½•åˆ—è¡¨
        keep_full_path: æ˜¯å¦ä¿ç•™å®Œæ•´è·¯å¾„
    """
    print("ğŸš€ å¼€å§‹å¤„ç†å’Œåˆå¹¶æ•°æ®é›†...")
    all_data = []
    source_stats = {}
    
    if image_base_dirs is None:
        image_base_dirs = [None] * len(datasets_info)
    
    # å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for i, ((json_path, desc), image_base_dir) in enumerate(zip(datasets_info, image_base_dirs)):
        try:
            processed_data = process_and_normalize_data(
                json_path, image_base_dir, desc, keep_full_path
            )
            all_data.extend(processed_data)
            source_stats[desc] = len(processed_data)
            print(f"âœ… {desc}: {len(processed_data)} æ¡æ•°æ®")
        except Exception as e:
            print(f"âŒ å¤„ç† {desc} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    if not all_data:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®å¯ä»¥åˆå¹¶ï¼")
    
    # æ•°æ®æ‰“ä¹±
    print(f"\nğŸ“Š æ€»è®¡æ ·æœ¬æ•°: {len(all_data)} æ¡ï¼Œæ­£åœ¨æ‰“ä¹±æ•°æ®...")
    random.seed(42)  # è®¾ç½®éšæœºç§å­ä»¥ä¾¿å¤ç°
    random.shuffle(all_data)
    
    # å†™å…¥è¾“å‡ºæ–‡ä»¶
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ æ­£åœ¨å°†æ•°æ®å†™å…¥åˆ° {output_path}...")
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in tqdm(all_data, desc="Writing final data"):
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
    print("\nğŸ‰ æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ åˆå¹¶åçš„æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“Š å„æ•°æ®æºç»Ÿè®¡:")
    for source, count in source_stats.items():
        percentage = (count / len(all_data)) * 100
        print(f"   - {source}: {count} æ¡ ({percentage:.1f}%)")
    
    return output_path

def fix_existing_alignment_data(input_file, output_file):
    """
    ä¿®å¤å·²ç»å­˜åœ¨çš„å¯¹é½æ•°æ®æ–‡ä»¶ï¼Œä¸ºå…¶æ·»åŠ <image>æ ‡ç­¾
    
    Args:
        input_file: è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºçš„ä¿®å¤åJSONLæ–‡ä»¶è·¯å¾„
    """
    print("ğŸ”§ ä¿®å¤ç°æœ‰æ•°æ®ï¼šä¸ºcaptionæ·»åŠ <image>æ ‡ç­¾...")
    
    fixed_count = 0
    total_count = 0
    
    with open(input_file, 'r', encoding='utf-8') as f_in, \
         open(output_file, 'w', encoding='utf-8') as f_out:
        
        for line in tqdm(f_in, desc="ä¿®å¤æ•°æ®"):
            if not line.strip():
                continue
            
            total_count += 1
            item = json.loads(line.strip())
            
            # æ£€æŸ¥captionæ˜¯å¦å·²ç»åŒ…å«<image>æ ‡ç­¾
            if 'caption' in item and not item['caption'].startswith('<image>'):
                # æ·»åŠ <image>æ ‡ç­¾åˆ°captionå¼€å¤´
                item['caption'] = '<image>\n' + item['caption']
                fixed_count += 1
            
            f_out.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… ä¿®å¤å®Œæˆï¼")
    print(f"   - æ€»æ ·æœ¬æ•°: {total_count}")
    print(f"   - ä¿®å¤æ ·æœ¬æ•°: {fixed_count}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶Stage1å¯¹é½è®­ç»ƒæ•°æ®é›†")
    parser.add_argument("--allava_json", type=str, 
                       default=r"D:\SoftData\VSCODE\é¡¶ä¼šè®ºæ–‡\MedPLIB-main\20250607\datasets\ALLaVA-Caption-LAION-4V.json",
                       help="ALLaVAæ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--llava_json", type=str,
                       default=r"D:\SoftData\VSCODE\é¡¶ä¼šè®ºæ–‡\MedPLIB-main\20250607\datasets\chat.json", 
                       help="LLaVAé¢„è®­ç»ƒæ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str,
                       default=r"D:\SoftData\VSCODE\é¡¶ä¼šè®ºæ–‡\MedPLIB-main\20250607\datasets\final_alignment_data_fixed.jsonl",
                       help="è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--keep_full_path", action="store_true", default=True,
                       help="æ˜¯å¦ä¿ç•™å®Œæ•´çš„å›¾ç‰‡è·¯å¾„ï¼ˆæ¨èï¼‰")
    parser.add_argument("--fix_existing", type=str, default="",
                       help="ä¿®å¤ç°æœ‰æ–‡ä»¶ï¼šæŒ‡å®šå·²å­˜åœ¨çš„JSONLæ–‡ä»¶è·¯å¾„æ¥æ·»åŠ <image>æ ‡ç­¾")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†ä¿®å¤ç°æœ‰æ–‡ä»¶çš„é€‰é¡¹
    if args.fix_existing:
        if not Path(args.fix_existing).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.fix_existing}")
            return 1
        
        # ç”Ÿæˆä¿®å¤åçš„æ–‡ä»¶å
        input_path = Path(args.fix_existing)
        output_path = input_path.parent / f"{input_path.stem}_fixed{input_path.suffix}"
        
        fix_existing_alignment_data(args.fix_existing, str(output_path))
        return 0
    
    # æ­£å¸¸çš„æ•°æ®åˆå¹¶æµç¨‹
    datasets_info = [
        (args.llava_json, "LLaVA-Pretrain"),
        (args.allava_json, "ALLaVA-4V")
    ]
    
    # æ‰§è¡Œåˆå¹¶
    try:
        output_file = validate_and_merge_datasets(
            datasets_info=datasets_info,
            output_path=args.output,
            keep_full_path=args.keep_full_path
        )
        print(f"\nâœ¨ ä»»åŠ¡å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_file}")
        print("ğŸ¯ ä¿®å¤è¯´æ˜: æ‰€æœ‰captionéƒ½å·²è‡ªåŠ¨æ·»åŠ <image>æ ‡ç­¾ï¼Œå¯ç›´æ¥ç”¨äºFILAè®­ç»ƒ")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())